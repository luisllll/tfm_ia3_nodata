{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully.\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.64      0.70        11\n",
      "           0       0.54      0.54      0.54        13\n",
      "           1       0.50      0.62      0.56         8\n",
      "\n",
      "    accuracy                           0.59        32\n",
      "   macro avg       0.61      0.60      0.60        32\n",
      "weighted avg       0.61      0.59      0.60        32\n",
      "\n",
      "[[7 3 1]\n",
      " [2 7 4]\n",
      " [0 3 5]]\n",
      "Support Vector Machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.55      0.63        11\n",
      "           0       0.56      0.69      0.62        13\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.66        32\n",
      "   macro avg       0.69      0.66      0.67        32\n",
      "weighted avg       0.67      0.66      0.66        32\n",
      "\n",
      "[[6 5 0]\n",
      " [2 9 2]\n",
      " [0 2 6]]\n",
      "Gradient Boosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.64      0.64        11\n",
      "           0       0.40      0.31      0.35        13\n",
      "           1       0.36      0.50      0.42         8\n",
      "\n",
      "    accuracy                           0.47        32\n",
      "   macro avg       0.47      0.48      0.47        32\n",
      "weighted avg       0.47      0.47      0.47        32\n",
      "\n",
      "[[7 2 2]\n",
      " [4 4 5]\n",
      " [0 4 4]]\n",
      "Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.55      0.57        11\n",
      "           0       0.43      0.46      0.44        13\n",
      "           1       0.50      0.50      0.50         8\n",
      "\n",
      "    accuracy                           0.50        32\n",
      "   macro avg       0.51      0.50      0.51        32\n",
      "weighted avg       0.51      0.50      0.50        32\n",
      "\n",
      "[[6 4 1]\n",
      " [4 6 3]\n",
      " [0 4 4]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Replace 'data.csv' with the filename of your CSV file\n",
    "csv_filename = 'entrenamiento.csv'\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "data = pd.read_csv(csv_filename)\n",
    "\n",
    "print(\"DataFrame loaded successfully.\")\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.drop(columns=['date'])\n",
    "data['prev_decision'] = data['prev_decision'].astype('category')\n",
    "X = data.drop(columns=['next_decision'])\n",
    "y = data['next_decision']\n",
    "\n",
    "# Undersample the categories to have the same number of observations\n",
    "class_neg1 = data[data.next_decision == -1]\n",
    "class_0 = data[data.next_decision == 0]\n",
    "class_1 = data[data.next_decision == 1]\n",
    "\n",
    "min_count = min(len(class_neg1), len(class_0), len(class_1))\n",
    "\n",
    "class_neg1_undersampled = resample(class_neg1, replace=False, n_samples=min_count, random_state=42)\n",
    "class_0_undersampled = resample(class_0, replace=False, n_samples=min_count, random_state=42)\n",
    "class_1_undersampled = resample(class_1, replace=False, n_samples=min_count, random_state=42)\n",
    "\n",
    "data_undersampled = pd.concat([class_neg1_undersampled, class_0_undersampled, class_1_undersampled])\n",
    "\n",
    "X_undersampled = data_undersampled.drop(columns=['next_decision'])\n",
    "y_undersampled = data_undersampled['next_decision']\n",
    "\n",
    "# Split the undersampled data into training and testing sets\n",
    "X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_undersampled, y_undersampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_us_scaled = scaler.fit_transform(X_train_us)\n",
    "X_test_us_scaled = scaler.transform(X_test_us)\n",
    "\n",
    "# Initialize models\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "svc = SVC(random_state=42)\n",
    "grad_boost = GradientBoostingClassifier(random_state=42)\n",
    "mlp = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "log_reg.fit(X_train_us_scaled, y_train_us)\n",
    "y_pred_log_reg = log_reg.predict(X_test_us_scaled)\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(y_test_us, y_pred_log_reg))\n",
    "print(confusion_matrix(y_test_us, y_pred_log_reg))\n",
    "\n",
    "# Train and evaluate Support Vector Machine (SVM)\n",
    "svc.fit(X_train_us_scaled, y_train_us)\n",
    "y_pred_svc = svc.predict(X_test_us_scaled)\n",
    "print(\"Support Vector Machine\")\n",
    "print(classification_report(y_test_us, y_pred_svc))\n",
    "print(confusion_matrix(y_test_us, y_pred_svc))\n",
    "\n",
    "# Train and evaluate Gradient Boosting Classifier\n",
    "grad_boost.fit(X_train_us_scaled, y_train_us)\n",
    "y_pred_grad_boost = grad_boost.predict(X_test_us_scaled)\n",
    "print(\"Gradient Boosting\")\n",
    "print(classification_report(y_test_us, y_pred_grad_boost))\n",
    "print(confusion_matrix(y_test_us, y_pred_grad_boost))\n",
    "\n",
    "# Train and evaluate Neural Network\n",
    "mlp.fit(X_train_us_scaled, y_train_us)\n",
    "y_pred_mlp = mlp.predict(X_test_us_scaled)\n",
    "print(\"Neural Network\")\n",
    "print(classification_report(y_test_us, y_pred_mlp))\n",
    "print(confusion_matrix(y_test_us, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Score: 0.6363333333333333\n",
      "Confusion Matrix for Best SVM:\n",
      "[[4 5 2]\n",
      " [3 7 3]\n",
      " [0 3 5]]\n",
      "\n",
      "Classification Report for Best SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.36      0.44        11\n",
      "           0       0.47      0.54      0.50        13\n",
      "           1       0.50      0.62      0.56         8\n",
      "\n",
      "    accuracy                           0.50        32\n",
      "   macro avg       0.51      0.51      0.50        32\n",
      "weighted avg       0.51      0.50      0.49        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_us_scaled, y_train_us)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "\n",
    "\n",
    "# Train the SVM model with the best parameters\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train_us_scaled, y_train_us)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best_svc = best_svc.predict(X_test_us_scaled)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "classification_rep_best_svc = classification_report(y_test_us, y_pred_best_svc)\n",
    "confusion_mat_best_svc = confusion_matrix(y_test_us, y_pred_best_svc)\n",
    "\n",
    "print(\"Confusion Matrix for Best SVM:\")\n",
    "print(confusion_mat_best_svc)\n",
    "print(\"\\nClassification Report for Best SVM:\")\n",
    "print(classification_rep_best_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance is only available for SVM with a linear kernel.\n",
      "Feature importance is only available for SVM with a linear kernel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Feature importance for SVM with linear kernel\n",
    "if best_svc.kernel == 'linear':\n",
    "    feature_importance = np.abs(best_svc.coef_[0])\n",
    "    feature_importance = feature_importance / np.sum(feature_importance)\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X_train_us.columns,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(\"Feature Importance for SVM with Linear Kernel:\")\n",
    "    print(feature_importance_df)\n",
    "else:\n",
    "    print(\"Feature importance is only available for SVM with a linear kernel.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure to have the correct SVM model with a linear kernel\n",
    "if best_svc.kernel == 'linear':\n",
    "    # Calculate feature importance\n",
    "    feature_importance = np.abs(best_svc.coef_[0])\n",
    "    feature_importance = feature_importance / np.sum(feature_importance)\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X_train_us.columns,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['feature'], feature_importance_df['importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance for SVM with Linear Kernel')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance is only available for SVM with a linear kernel.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
